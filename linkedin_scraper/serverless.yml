service: linkedin-job-scraper

plugins:
  - serverless-python-requirements

custom:
  pythonRequirements:
    pythonBin: python3
    dockerizePip: true
    slim: true
    strip: false

provider:
  name: aws
  runtime: python3.9
  region: us-east-1
  stage: dev
  timeout: 600  # 10 minutes Lambda timeout (scheduled runs unaffected by API limits)
  memorySize: 1024  # 1GB memory
  deploymentBucket:
    name: linkedin-job-scraper-deployment-bucket
    serverSideEncryption: AES256
  environment:
    LINKEDIN_EMAIL: ${env:LINKEDIN_EMAIL}
    LINKEDIN_PASSWORD: ${env:LINKEDIN_PASSWORD}
    JOBS_BUCKET: ${self:service}-${self:provider.stage}-jobs
  iam:
    role:
      statements:
        - Effect: Allow
          Action:
            - s3:GetObject
            - s3:PutObject
            - s3:DeleteObject
            - s3:CreateBucket
            - s3:ListBucket
            - s3:GetBucketLocation
          Resource: 
            - "arn:aws:s3:::${self:service}-${self:provider.stage}-jobs/*"
            - "arn:aws:s3:::${self:service}-${self:provider.stage}-jobs"
        - Effect: Allow
          Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
          Resource: "*"
        - Effect: Allow
          Action:
            - ssm:GetParameter
            - ssm:PutParameter
            - ssm:DeleteParameter
          Resource: "*"
        - Effect: Allow
          Action:
            - events:DescribeRule
            - events:PutRule
            - events:DeleteRule
            - events:PutTargets
            - events:RemoveTargets
            - events:ListTargetsByRule
          Resource: "*"

functions:
  api:
    handler: lambda_handler.handler
    timeout: 600  # Keep Lambda at 10 minutes; API integration will cap during testing
    events:
      - http:  # REST API (supports >30s once quota is approved)
          path: /{proxy+}
          method: ANY
          cors: true
          integration: lambda-proxy
      - http:  # REST API root
          path: /
          method: ANY
          cors: true
          integration: lambda-proxy
      # Main scraping every hour (all 7 batches) - accumulates data
      - schedule:
          rate: rate(1 hour)
          input:
            batch_number: 1
            batch_size: 18
      - schedule:
          rate: rate(1 hour)
          input:
            batch_number: 2
            batch_size: 18
      - schedule:
          rate: rate(1 hour)
          input:
            batch_number: 3
            batch_size: 18
      - schedule:
          rate: rate(1 hour)
          input:
            batch_number: 4
            batch_size: 18
      - schedule:
          rate: rate(1 hour)
          input:
            batch_number: 5
            batch_size: 18
      - schedule:
          rate: rate(1 hour)
          input:
            batch_number: 6
            batch_size: 18
      - schedule:
          rate: rate(1 hour)
          input:
            batch_number: 7
            batch_size: 18

resources:
  Resources:
    JobScraperBucket:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: ${self:service}-${self:provider.stage}-jobs
        PublicAccessBlockConfiguration:
          BlockPublicAcls: true
          BlockPublicPolicy: true
          IgnorePublicAcls: true
          RestrictPublicBuckets: true